# Fine-tune an LM on the enron text dataset.

dataset_args:
  dataset_path: AUEB-NLP/ecthr_cases
  dataset_config: "alleged-violation-prediction"
  cache_dir: "/app/cache"
  cache_dir: /tmp/hf_datasets
  dataset_mode: undefended
  dataset_config: violation-predictions
  split: train
  sample_duplication_rate: 1

trainer_args:
  save_steps: 750
  callback_after_n_steps: 750
  num_train_epochs: 4
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8

model_args:
  architecture: gpt2
  pre_trained: True   # Start from a pre-trained checkpoint

ner_args:
  ner: flair
  ner_model: flair/ner-english-ontonotes-large
  anon_token: <MASK>
  anonymize: False
